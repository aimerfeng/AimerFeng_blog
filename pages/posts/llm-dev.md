---
title: LLM åº”ç”¨å¼€å‘å®æˆ˜ - ä» Prompt å·¥ç¨‹åˆ° RAG ç³»ç»Ÿ
date: 2024-12-18
description: æ·±å…¥å­¦ä¹ å¤§è¯­è¨€æ¨¡å‹åº”ç”¨å¼€å‘ï¼Œæ¶µç›– Prompt å·¥ç¨‹ã€RAG æ¶æ„ã€å‘é‡æ•°æ®åº“å’Œç”Ÿäº§éƒ¨ç½²
tags: [ai, llm, python]
---

# LLM åº”ç”¨å¼€å‘å®æˆ˜

å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ­£åœ¨æ”¹å˜è½¯ä»¶å¼€å‘çš„æ–¹å¼ã€‚æœ¬æ–‡å°†åˆ†äº«æˆ‘åœ¨ LLM åº”ç”¨å¼€å‘ä¸­çš„å®è·µç»éªŒï¼Œä»åŸºç¡€çš„ Prompt å·¥ç¨‹åˆ°å¤æ‚çš„ RAG ç³»ç»Ÿã€‚

## Prompt å·¥ç¨‹åŸºç¡€

### 1. ç»“æ„åŒ– Prompt

å¥½çš„ Prompt åº”è¯¥æ¸…æ™°ã€å…·ä½“ã€æœ‰ç»“æ„ï¼š

```python
from openai import OpenAI

client = OpenAI()

def structured_prompt(task: str, context: str, requirements: list[str]) -> str:
    """æ„å»ºç»“æ„åŒ– Prompt"""
    requirements_text = "\n".join(f"- {r}" for r in requirements)
    
    prompt = f"""
# ä»»åŠ¡
{task}

# èƒŒæ™¯ä¿¡æ¯
{context}

# è¦æ±‚
{requirements_text}

# è¾“å‡ºæ ¼å¼
è¯·ä»¥ JSON æ ¼å¼è¿”å›ç»“æœã€‚
"""
    return prompt

# ä½¿ç”¨ç¤ºä¾‹
prompt = structured_prompt(
    task="åˆ†æç”¨æˆ·è¯„è®ºçš„æƒ…æ„Ÿå€¾å‘",
    context="è¿™æ˜¯ä¸€ä¸ªç”µå•†å¹³å°çš„å•†å“è¯„è®º",
    requirements=[
        "è¯†åˆ«æ­£é¢ã€è´Ÿé¢ã€ä¸­æ€§æƒ…æ„Ÿ",
        "æå–å…³é”®è¯",
        "ç»™å‡ºç½®ä¿¡åº¦åˆ†æ•°"
    ]
)

response = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": prompt}],
    response_format={"type": "json_object"}
)
```

### 2. Few-Shot Learning

é€šè¿‡ç¤ºä¾‹å¼•å¯¼æ¨¡å‹è¾“å‡ºï¼š

```python
def few_shot_prompt(examples: list[dict], query: str) -> str:
    """æ„å»º Few-Shot Prompt"""
    examples_text = ""
    for ex in examples:
        examples_text += f"""
è¾“å…¥: {ex['input']}
è¾“å‡º: {ex['output']}
---
"""
    
    return f"""
è¯·æ ¹æ®ä»¥ä¸‹ç¤ºä¾‹ï¼Œå®Œæˆä»»åŠ¡ï¼š

{examples_text}

ç°åœ¨è¯·å¤„ç†ï¼š
è¾“å…¥: {query}
è¾“å‡º:"""

# ç¤ºä¾‹ï¼šä»£ç ç¿»è¯‘
examples = [
    {
        "input": "Python: print('Hello')",
        "output": "JavaScript: console.log('Hello')"
    },
    {
        "input": "Python: len(arr)",
        "output": "JavaScript: arr.length"
    }
]

prompt = few_shot_prompt(examples, "Python: for i in range(10):")
```

### 3. Chain of Thought (CoT)

è®©æ¨¡å‹å±•ç¤ºæ¨ç†è¿‡ç¨‹ï¼š

```python
def cot_prompt(problem: str) -> str:
    """Chain of Thought Prompt"""
    return f"""
è¯·è§£å†³ä»¥ä¸‹é—®é¢˜ï¼Œå¹¶å±•ç¤ºä½ çš„æ€è€ƒè¿‡ç¨‹ï¼š

é—®é¢˜ï¼š{problem}

è¯·æŒ‰ä»¥ä¸‹æ­¥éª¤å›ç­”ï¼š
1. é¦–å…ˆï¼Œç†è§£é—®é¢˜çš„å…³é”®ç‚¹
2. ç„¶åï¼Œåˆ†æå¯èƒ½çš„è§£å†³æ–¹æ¡ˆ
3. æ¥ç€ï¼Œé€‰æ‹©æœ€ä½³æ–¹æ¡ˆå¹¶è§£é‡ŠåŸå› 
4. æœ€åï¼Œç»™å‡ºæœ€ç»ˆç­”æ¡ˆ

è®©æˆ‘ä»¬ä¸€æ­¥æ­¥æ€è€ƒï¼š
"""

# ä½¿ç”¨
response = client.chat.completions.create(
    model="gpt-4",
    messages=[{
        "role": "user",
        "content": cot_prompt("å¦‚ä½•è®¾è®¡ä¸€ä¸ªé«˜å¹¶å‘çš„æ¶ˆæ¯é˜Ÿåˆ—ç³»ç»Ÿï¼Ÿ")
    }]
)
```

## RAG ç³»ç»Ÿå¼€å‘

RAGï¼ˆRetrieval-Augmented Generationï¼‰é€šè¿‡æ£€ç´¢å¢å¼ºç”Ÿæˆï¼Œè®© LLM èƒ½å¤Ÿè®¿é—®å¤–éƒ¨çŸ¥è¯†ã€‚

### æ¶æ„æ¦‚è§ˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RAG Pipeline                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ Document â”‚â”€â”€â”€â–¶â”‚ Chunking â”‚â”€â”€â”€â–¶â”‚ Embeddingâ”‚      â”‚
â”‚  â”‚  Loader  â”‚    â”‚          â”‚    â”‚          â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                       â”‚             â”‚
â”‚                                       â–¼             â”‚
â”‚                               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚                               â”‚ Vector Store â”‚      â”‚
â”‚                               â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                      â”‚              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚              â”‚
â”‚  â”‚  Query   â”‚â”€â”€â”€â–¶â”‚ Retrieverâ”‚â—€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                       â”‚                             â”‚
â”‚                       â–¼                             â”‚
â”‚               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚               â”‚   LLM + RAG  â”‚                      â”‚
â”‚               â”‚   Response   â”‚                      â”‚
â”‚               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### å®Œæ•´å®ç°

```python
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain_community.document_loaders import DirectoryLoader, TextLoader

# 1. åŠ è½½æ–‡æ¡£
def load_documents(directory: str):
    """åŠ è½½ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡æ¡£"""
    loader = DirectoryLoader(
        directory,
        glob="**/*.md",
        loader_cls=TextLoader
    )
    return loader.load()

# 2. æ–‡æ¡£åˆ†å—
def split_documents(documents, chunk_size=1000, chunk_overlap=200):
    """å°†æ–‡æ¡£åˆ†å‰²æˆå°å—"""
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", " ", ""]
    )
    return splitter.split_documents(documents)

# 3. åˆ›å»ºå‘é‡å­˜å‚¨
def create_vector_store(chunks, persist_directory="./chroma_db"):
    """åˆ›å»ºå¹¶æŒä¹…åŒ–å‘é‡å­˜å‚¨"""
    embeddings = OpenAIEmbeddings()
    
    vectorstore = Chroma.from_documents(
        documents=chunks,
        embedding=embeddings,
        persist_directory=persist_directory
    )
    
    return vectorstore

# 4. æ„å»º RAG Chain
def create_rag_chain(vectorstore):
    """åˆ›å»º RAG é—®ç­”é“¾"""
    llm = ChatOpenAI(model="gpt-4", temperature=0)
    
    retriever = vectorstore.as_retriever(
        search_type="similarity",
        search_kwargs={"k": 4}
    )
    
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True
    )
    
    return qa_chain

# å®Œæ•´æµç¨‹
def build_rag_system(docs_directory: str):
    """æ„å»ºå®Œæ•´çš„ RAG ç³»ç»Ÿ"""
    # åŠ è½½æ–‡æ¡£
    print("ğŸ“š åŠ è½½æ–‡æ¡£...")
    documents = load_documents(docs_directory)
    
    # åˆ†å—
    print("âœ‚ï¸ åˆ†å‰²æ–‡æ¡£...")
    chunks = split_documents(documents)
    
    # åˆ›å»ºå‘é‡å­˜å‚¨
    print("ğŸ”¢ åˆ›å»ºå‘é‡ç´¢å¼•...")
    vectorstore = create_vector_store(chunks)
    
    # åˆ›å»º RAG Chain
    print("ğŸ”— æ„å»º RAG Chain...")
    qa_chain = create_rag_chain(vectorstore)
    
    print("âœ… RAG ç³»ç»Ÿæ„å»ºå®Œæˆ!")
    return qa_chain

# ä½¿ç”¨
rag = build_rag_system("./knowledge_base")
result = rag.invoke({"query": "å¦‚ä½•éƒ¨ç½²æ™ºèƒ½åˆçº¦ï¼Ÿ"})
print(result["result"])
```

## å‘é‡æ•°æ®åº“é€‰å‹

### å¸¸ç”¨å‘é‡æ•°æ®åº“å¯¹æ¯”

| æ•°æ®åº“ | ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ |
|--------|------|----------|
| Chroma | è½»é‡ã€æ˜“ç”¨ | åŸå‹å¼€å‘ã€å°è§„æ¨¡ |
| Pinecone | æ‰˜ç®¡æœåŠ¡ã€é«˜æ€§èƒ½ | ç”Ÿäº§ç¯å¢ƒã€å¤§è§„æ¨¡ |
| Milvus | å¼€æºã€åŠŸèƒ½ä¸°å¯Œ | è‡ªæ‰˜ç®¡ã€ä¼ä¸šçº§ |
| Weaviate | GraphQL API | å¤æ‚æŸ¥è¯¢éœ€æ±‚ |
| Qdrant | Rust å®ç°ã€é«˜æ€§èƒ½ | é«˜æ€§èƒ½éœ€æ±‚ |

### Pinecone é›†æˆç¤ºä¾‹

```python
from pinecone import Pinecone, ServerlessSpec
from langchain_pinecone import PineconeVectorStore
from langchain_openai import OpenAIEmbeddings

# åˆå§‹åŒ– Pinecone
pc = Pinecone(api_key="your-api-key")

# åˆ›å»ºç´¢å¼•
index_name = "my-rag-index"
if index_name not in pc.list_indexes().names():
    pc.create_index(
        name=index_name,
        dimension=1536,  # OpenAI embedding ç»´åº¦
        metric="cosine",
        spec=ServerlessSpec(
            cloud="aws",
            region="us-east-1"
        )
    )

# åˆ›å»ºå‘é‡å­˜å‚¨
embeddings = OpenAIEmbeddings()
vectorstore = PineconeVectorStore(
    index=pc.Index(index_name),
    embedding=embeddings,
    text_key="text"
)

# æ·»åŠ æ–‡æ¡£
vectorstore.add_documents(chunks)

# ç›¸ä¼¼åº¦æœç´¢
results = vectorstore.similarity_search("æ™ºèƒ½åˆçº¦å®‰å…¨", k=5)
```

## æµå¼è¾“å‡º

æå‡ç”¨æˆ·ä½“éªŒçš„å…³é”®ï¼š

```python
from openai import OpenAI

client = OpenAI()

def stream_response(prompt: str):
    """æµå¼è¾“å‡ºå“åº”"""
    stream = client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}],
        stream=True
    )
    
    for chunk in stream:
        if chunk.choices[0].delta.content:
            yield chunk.choices[0].delta.content

# ä½¿ç”¨
for text in stream_response("è§£é‡Šä»€ä¹ˆæ˜¯åŒºå—é“¾"):
    print(text, end="", flush=True)
```

### FastAPI æµå¼æ¥å£

```python
from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from openai import OpenAI

app = FastAPI()
client = OpenAI()

@app.post("/chat/stream")
async def chat_stream(message: str):
    async def generate():
        stream = client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": message}],
            stream=True
        )
        
        for chunk in stream:
            if chunk.choices[0].delta.content:
                yield f"data: {chunk.choices[0].delta.content}\n\n"
        
        yield "data: [DONE]\n\n"
    
    return StreamingResponse(
        generate(),
        media_type="text/event-stream"
    )
```

## æˆæœ¬ä¼˜åŒ–

### 1. Token è®¡ç®—å’Œé¢„ç®—æ§åˆ¶

```python
import tiktoken

def count_tokens(text: str, model: str = "gpt-4") -> int:
    """è®¡ç®—æ–‡æœ¬çš„ token æ•°é‡"""
    encoding = tiktoken.encoding_for_model(model)
    return len(encoding.encode(text))

def estimate_cost(input_tokens: int, output_tokens: int, model: str = "gpt-4") -> float:
    """ä¼°ç®— API è°ƒç”¨æˆæœ¬"""
    # GPT-4 ä»·æ ¼ï¼ˆç¤ºä¾‹ï¼Œå®é™…ä»·æ ¼è¯·æŸ¥çœ‹å®˜æ–¹ï¼‰
    prices = {
        "gpt-4": {"input": 0.03, "output": 0.06},
        "gpt-3.5-turbo": {"input": 0.0015, "output": 0.002}
    }
    
    price = prices.get(model, prices["gpt-4"])
    cost = (input_tokens / 1000 * price["input"]) + \
           (output_tokens / 1000 * price["output"])
    
    return cost

# ä½¿ç”¨
text = "è¿™æ˜¯ä¸€æ®µæµ‹è¯•æ–‡æœ¬..."
tokens = count_tokens(text)
print(f"Token æ•°é‡: {tokens}")
print(f"é¢„ä¼°æˆæœ¬: ${estimate_cost(tokens, 500):.4f}")
```

### 2. ç¼“å­˜ç­–ç•¥

```python
import hashlib
import json
from functools import lru_cache
import redis

# å†…å­˜ç¼“å­˜
@lru_cache(maxsize=1000)
def cached_completion(prompt_hash: str) -> str:
    """åŸºäº LRU çš„å†…å­˜ç¼“å­˜"""
    # å®é™…è°ƒç”¨ API
    pass

# Redis ç¼“å­˜
class LLMCache:
    def __init__(self, redis_url: str = "redis://localhost:6379"):
        self.redis = redis.from_url(redis_url)
        self.ttl = 3600 * 24  # 24 å°æ—¶
    
    def _hash_prompt(self, prompt: str, model: str) -> str:
        content = f"{model}:{prompt}"
        return hashlib.sha256(content.encode()).hexdigest()
    
    def get(self, prompt: str, model: str) -> str | None:
        key = self._hash_prompt(prompt, model)
        cached = self.redis.get(key)
        return cached.decode() if cached else None
    
    def set(self, prompt: str, model: str, response: str):
        key = self._hash_prompt(prompt, model)
        self.redis.setex(key, self.ttl, response)

# ä½¿ç”¨
cache = LLMCache()

def cached_chat(prompt: str, model: str = "gpt-4") -> str:
    # æ£€æŸ¥ç¼“å­˜
    cached = cache.get(prompt, model)
    if cached:
        return cached
    
    # è°ƒç”¨ API
    response = client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": prompt}]
    )
    result = response.choices[0].message.content
    
    # å­˜å…¥ç¼“å­˜
    cache.set(prompt, model, result)
    
    return result
```

## ç”Ÿäº§éƒ¨ç½²æœ€ä½³å®è·µ

### 1. é”™è¯¯å¤„ç†å’Œé‡è¯•

```python
from tenacity import retry, stop_after_attempt, wait_exponential
from openai import RateLimitError, APIError

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=4, max=60),
    retry=lambda e: isinstance(e, (RateLimitError, APIError))
)
def robust_completion(prompt: str) -> str:
    """å¸¦é‡è¯•æœºåˆ¶çš„ API è°ƒç”¨"""
    response = client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}]
    )
    return response.choices[0].message.content
```

### 2. ç›‘æ§å’Œæ—¥å¿—

```python
import logging
import time
from dataclasses import dataclass
from datetime import datetime

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class LLMMetrics:
    timestamp: datetime
    model: str
    input_tokens: int
    output_tokens: int
    latency_ms: float
    success: bool
    error: str | None = None

class LLMMonitor:
    def __init__(self):
        self.metrics: list[LLMMetrics] = []
    
    def record(self, metric: LLMMetrics):
        self.metrics.append(metric)
        logger.info(f"LLM Call: model={metric.model}, "
                   f"tokens={metric.input_tokens}+{metric.output_tokens}, "
                   f"latency={metric.latency_ms:.0f}ms")
    
    def get_stats(self) -> dict:
        if not self.metrics:
            return {}
        
        total_tokens = sum(m.input_tokens + m.output_tokens for m in self.metrics)
        avg_latency = sum(m.latency_ms for m in self.metrics) / len(self.metrics)
        success_rate = sum(1 for m in self.metrics if m.success) / len(self.metrics)
        
        return {
            "total_calls": len(self.metrics),
            "total_tokens": total_tokens,
            "avg_latency_ms": avg_latency,
            "success_rate": success_rate
        }

monitor = LLMMonitor()

def monitored_completion(prompt: str, model: str = "gpt-4") -> str:
    start = time.time()
    
    try:
        response = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}]
        )
        
        metric = LLMMetrics(
            timestamp=datetime.now(),
            model=model,
            input_tokens=response.usage.prompt_tokens,
            output_tokens=response.usage.completion_tokens,
            latency_ms=(time.time() - start) * 1000,
            success=True
        )
        monitor.record(metric)
        
        return response.choices[0].message.content
        
    except Exception as e:
        metric = LLMMetrics(
            timestamp=datetime.now(),
            model=model,
            input_tokens=0,
            output_tokens=0,
            latency_ms=(time.time() - start) * 1000,
            success=False,
            error=str(e)
        )
        monitor.record(metric)
        raise
```

## æ€»ç»“

LLM åº”ç”¨å¼€å‘çš„å…³é”®è¦ç‚¹ï¼š

- ğŸ“ **Prompt å·¥ç¨‹** - ç»“æ„åŒ–ã€Few-Shotã€CoT
- ğŸ” **RAG ç³»ç»Ÿ** - è®© LLM è®¿é—®å¤–éƒ¨çŸ¥è¯†
- ğŸ’¾ **å‘é‡æ•°æ®åº“** - é€‰æ‹©é€‚åˆçš„å­˜å‚¨æ–¹æ¡ˆ
- ğŸŒŠ **æµå¼è¾“å‡º** - æå‡ç”¨æˆ·ä½“éªŒ
- ğŸ’° **æˆæœ¬ä¼˜åŒ–** - ç¼“å­˜ã€Token æ§åˆ¶
- ğŸ›¡ï¸ **ç”Ÿäº§å°±ç»ª** - é”™è¯¯å¤„ç†ã€ç›‘æ§

å¸Œæœ›è¿™ç¯‡æ–‡ç« å¯¹ä½ çš„ LLM åº”ç”¨å¼€å‘æœ‰æ‰€å¸®åŠ©ï¼

---

<div class="mt-8 p-4 glass-card">
  <p class="text-sm op-75">
    ğŸš€ å¦‚æœä½ æ­£åœ¨å¼€å‘ LLM åº”ç”¨ï¼Œæ¬¢è¿äº¤æµè®¨è®ºï¼
  </p>
</div>
